{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Up to this point, we have been applying statistical and probabilistic rules to anticipate events (Probability) and to compare data (Inference). We now look beyond to the real power of statistics which is **Prediction**.\n",
    "\n",
    "In order to predict we need to model data. We need to create an imperfect and more simplistic view of our data that could be extrapolated to unseen aspects of our outcome variable.\n",
    "\n",
    "Linear regression is a simple yet powerful modeling technique that permit us to model an outcome as a function on *one* or *multiple* predictor or explanatory variable.\n",
    "\n",
    "With simple linear regressions we can explore the **strength of the association** between a quatitative outcome and another quantitative variable the predictor. We can predict values from the model or/and make inferences (statistical tests of associations) regarding associations in the dataset.\n",
    "\n",
    "### How does it work\n",
    "\n",
    "The main Goal is to Find a linear function based on X to best yield Y.\n",
    "\n",
    "\n",
    "X = “covariate” = “feature” = “predictor” = “regressor” = “independent variable”\n",
    "\n",
    "Y = “response variable” = “outcome” = “dependent variable”\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$r(x) = E (Y|X = x)$$\n",
    "\n",
    "Goal: Estimate function r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear Regression\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1X + \\epsilon_i, for i = 1,...,n$$\n",
    "\n",
    "![title](L_reg.png)\n",
    "\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are two unknown constants that represent the intercept and slope, also known as **coefficients** or **parameters**, n is the number of observations in the dataset, and $\\epsilon$ is the **error** term\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- c(173, 169, 176, 166, 161, 164, 160, 158, 180, 187)\n",
    "y <- c(80, 68, 72, 75, 70, 65, 62, 60, 85, 92) # plot scatterplot and the regression line\n",
    "\n",
    "mod1 <- lm(y ~ x)\n",
    "plot(x, y, xaxt=\"n\")\n",
    "axis(side=1, at = seq(150, 190, by = 1))\n",
    "axis(side=2, at= seq(60,95,5))\n",
    "abline(mod1, lwd=2)\n",
    "grid(nx = NULL,ny = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\beta_0$ Intercept: Predicted value when x = 0\n",
    "\n",
    "### $\\beta_1$ Slope: Predicted increase of y, for a unit increase in x\n",
    "\n",
    "However, we never know the true coefficients $\\beta_0$ and $\\beta_1$ from the population, but we can estimate them using the sample parameters $\\hat{\\beta_0}$ $\\hat{\\beta_1}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lm() is the function in R that calculates the sample parameters from the data and find the \"best\" coefficients. What is not captured by the coefficients (the noise) is captured by the residuals\n",
    "\n",
    "Residual $$\\epsilon = Y_i - \\bar{Y}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate residuals and predicted values\n",
    "plot(x, y, xlim=c(min(x)-5, max(x)+5), ylim=c(min(y)-10, max(y)+10))\n",
    "abline(mod1, lwd=2)\n",
    "res <- signif(residuals(mod1), 5)\n",
    "pre <- predict(mod1) # plot distances between points and the regression line\n",
    "segments(x, y, x, pre, col=\"red\")\n",
    "\n",
    "# add labels (residual values) to points\n",
    "#install.packages(\"calibrate\")\n",
    "library(calibrate)\n",
    "textxy(x, y, res, cex=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get that function? Using the Least Squares criteria, which minimizes the sum of squares residuals. There is only one Least Squares regression line.\n",
    "\n",
    "Let's review an example taken from the book by Baumer, Benjamin S. \"Modern Data Science with R\", that helps to demonstrate the general applications of linear regressions. \n",
    "\n",
    "This dataset was collected by the The Pioneer Valley Planning Commission (PVPC) which collected data north of Chestnut Street in Florence, Massachusetts for a ninety day period. Data collectors set up a laser sensor that recorded when a rail-trail user passed the data collection station.\n",
    " \n",
    "First lest install two libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(mosaic)\n",
    "library(tibble)\n",
    "glimpse(RailTrail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore first the relationship between daily ridership (i.e., the number of riders and walkers who use the bike path on any given day) and a collection of explanatory variables, including the temperature, rainfall, cloud cover, and day of the week.\n",
    "\n",
    "Let's start with one single qualitative variable, high temperature. \n",
    "\n",
    "We can visualize such relationship by plotting the scatterplot\n",
    "\n",
    "Remember that our independent variable or our predictor is the variable that can randomly change (this one is our x axis). The Response variable, is the variable that changes (or not) as a function of the predictor (Y axis). In our example, the dependent variable is the number of riders and walkers who use the bike a day (Volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(RailTrail$hightemp, RailTrail$volume) ##What do you see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate the coefficients from this linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod <- lm(volume ~ hightemp, data = RailTrail) \n",
    "coef(mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remember the intercept is $\\hat{\\beta_0}$ from the linear equation and $\\hat{\\beta_1}$ is the slope of the line. We can conclude two things:\n",
    "\n",
    "1. In the coldest days, when the high temperature was 0 Fahrenheit, the mean rider frequency is -17 (this is not possible). Probably at these extreme temperatures the monitoring equipment was not working very well.\n",
    "\n",
    "2. This is the most important extrapolation from the linear model which is that for each increase in 1 degree Fahrenheit, we increase about 5.7 riders a day. \n",
    "\n",
    "we can visualize this model scatterplot also : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModel(mod, system = \"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What else we can get from a linear regression?\n",
    "\n",
    "How does our model compares with a null model ? What is the null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(1,2))\n",
    "plot(RailTrail$hightemp, RailTrail$volume)\n",
    "abline(h = 375.4, lwd=2)\n",
    "pre2 = sapply(RailTrail$volume, function(x) x-375.4)\n",
    "res_mod2 <- signif(pre2, 5)\n",
    "segments(RailTrail$hightemp, RailTrail$volume, RailTrail$hightemp, 375.4, col=\"red\")\n",
    "#textxy(RailTrail$hightemp, RailTrail$volume, res_mod2, cex=0.7)\n",
    "title(main=\"Null\")\n",
    "    \n",
    "plot(RailTrail$hightemp, RailTrail$volume)\n",
    "abline(mod, lwd=2)\n",
    "res_mod <- signif(residuals(mod), 5)\n",
    "pre <- predict(mod) # plot distances between points and the regression line\n",
    "segments(RailTrail$hightemp, RailTrail$volume, RailTrail$hightemp, pre, col=\"red\")\n",
    "title(main=\"LM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to asses how well fit our linear model to our data we need to obtain a measurement of strenght of the linear relationship, which is our correlation coefficient r\n",
    "\n",
    "## Correlation\n",
    "\n",
    "The main difference between correlation and simple linear regression is that in correlation we are not interested in investigating the effect of our independent variable on the dependent variable, but rather to understand the strenght of the association between two continuous variables.\n",
    "\n",
    "## Pearson Correlation\n",
    "\n",
    "Pearson's correlation coefficient is the covariance of two continuous variables divided by the product of their standard deviations.\n",
    "\n",
    "$$r = \\frac{\\Sigma(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma(x_i - \\bar{x})^2\\Sigma(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "$$r = \\frac{n\\Sigma(x_i*y_i)-\\Sigma(x_i)*\\Sigma(y_i)}{\\sqrt{n*\\Sigma x_i^2 - (\\Sigma x_i)^2}\\sqrt{n*\\Sigma y_i^2 - (\\Sigma y_i)^2}}$$\n",
    "\n",
    "or Equally\n",
    "\n",
    "$$r = \\displaystyle \\frac{1}{n-1} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s_x}\\right) \\left( \\frac{y_i - \\bar{y}}{s_y}\\right)$$\n",
    "\n",
    "What does the two summed factors remind you off? \n",
    "\n",
    "Z-scores right?, what we are doing here is standarizing the two variables to be equate $\\hat{\\beta_0}$ =0 and  $\\hat{\\beta_1}$ = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvol = RailTrail$volume\n",
    "ytemp = RailTrail$hightemp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcal = data.frame(xvol,ytemp)\n",
    "rcal$X2 = rcal$xvol^2\n",
    "rcal$Y2 = rcal$ytemp^2\n",
    "rcal$XY = rcal$xvol * rcal$ytemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(rcal)\n",
    "totals = colSums(rcal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "den = nrow(rcal)*totals[5] - totals[1] * totals[2]\n",
    "num = (sqrt(nrow(rcal)*totals[3]-(totals[1])^2) *\n",
    "         sqrt(nrow(rcal)*totals[4]-(totals[2])^2) )\n",
    "den/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5825719^2\n",
    "##R2 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_res <- cor.test(RailTrail$volume, RailTrail$hightemp, \n",
    "                method = \"pearson\")\n",
    "cor_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = lm(RailTrail$hightemp~RailTrail$volume)\n",
    "summary(reg)\n",
    "coeff=coefficients(reg)\n",
    "eq = paste0(\"y = \", round(coeff[1],1), \"*x \", round(coeff[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(RailTrail$volume, RailTrail$hightemp,main=eq)\n",
    "abline(reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficient is ranges between -1 and 1:\n",
    "\n",
    "-  -1 indicates a strong negative correlation : this means that every time x increases, y decreases (left panel figure)\n",
    "-  0 means that there is no association between the two variables (x and y) (middle panel figure)\n",
    "-  1 indicates a strong positive correlation : this means that y increases with x (right panel figure)\n",
    "\n",
    "![title](Correlation.png)\n",
    "Image taken From: [http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember: that having an association it does not mean that the association is real and that the variables are affecting each other.\n",
    "\n",
    "# Correlation doesn't imply causation\n",
    "\n",
    "Some funny examples [http://www.tylervigen.com/spurious-correlations](http://www.tylervigen.com/spurious-correlations) \n",
    "\n",
    "But it also happens in real science: see first example on wikipedia [https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In linear models we can also measure the goodness of fit:\n",
    "\n",
    "Another way of examining whether the regression line shows a relationship between the two variables is to determine how well the line fits the observed data points.\n",
    "\n",
    "We can use the data contained on the error predictors and generate a single statistic called $\\large {R^2}$ or goodness of fit of the regression line\n",
    "\n",
    "$\\large {R^2}$ is a measure the proportion of variation in the response variable (y) that is explained by the model in a similar fashion (Baumer 2017).\n",
    "\n",
    "$\\large {R^2}$ is a proportion of variation and it ranges from 0 to 1, for simple linear regressions \n",
    "\n",
    "$$\\large {R^2} = 1- \\frac {SSE}{SST} = \\frac {SSM}{SST}$$\n",
    "\n",
    "$$               = 1- \\frac {\\sum_{i=1}^n (y_i - \\hat{y})^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2} = \\frac {SSM}{SST}$$\n",
    "\n",
    "$$           = 1- \\frac {SSE}{(n-1)Var(y)}$$\n",
    "\n",
    "\n",
    "where SSE is the sum of the squared residuals, SSM is the sum of the squares attributed to the model, and SST is the total sum of the squares. Let’s calculate these values for the rail trail example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- nrow(RailTrail) \n",
    "SST <- var(~volume, data = RailTrail) * (n - 1) \n",
    "SSE <- var(residuals(reg)) * (n - 1) \n",
    "1 - SSE / SST\n",
    "\n",
    "rsquared(reg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We say that the regression model based on average daily temperature explained about 34% of the variation in daily ridership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical predictor variables\n",
    "\n",
    "Following our example of usage of rail trail, we were wondering if maybe there was a difference in the relationship between volume and the time of the week (weekdays or weekends). looking at the data set we see that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(RailTrail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekday is a binary variable (whether it was a weekday or not) Other examples of binary variables are gender, we can also have categorial variables that are not binary (days of the week, socioeconomical status -low,middle,high- etc.\n",
    "\n",
    "our linear model becomes:\n",
    "\n",
    "$$\\hat{Volume} = \\beta_0+\\beta_1 * weekday $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the coefficients\n",
    "coef(lm(volume ~ weekday, data = RailTrail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(volume ~ weekday, data = RailTrail)\n",
    "diff(mean(volume ~ weekday, data = RailTrail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result can be interpreted as we expect 80 less riders on weekdays (which was 1 in our variable) than on the weekend. \n",
    "\n",
    "We can transform our variable to make it easier to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RailTrail <- RailTrail %>% mutate(day = ifelse(weekday == 1, \"weekday\", \"weekend/holiday\"))\n",
    "coef(lm(volume ~ day, data = RailTrail))\n",
    "glimpse(RailTrail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(lm(volume ~ day, data = RailTrail))\n",
    "rsquared(lm(volume ~ day, data = RailTrail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the interpretation of the model remains the same: On a weekday, 80 more riders are expected on a weekend or holiday than during weekdays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "## One of the most powerfull features of the regression models is the power to use the model to predict new data.\n",
    "### We can use our model to predict the volume of people on the trail at any temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = lm(volume~hightemp, data = RailTrail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftoPred = data.frame(hightemp=3)\n",
    "predict(reg2, newdata=dftoPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate the model??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, one of the most frequently used error estimators for regression models is the RMSE(root mean squared error), which is a measurement of quality of the fit for a particular model. This value represents the deviation of the residuals compared to the model.\n",
    "\n",
    "The idea is to measure how close is the predicted response for each observation to the true response value for that observation.\n",
    "\n",
    "$$ MSE = \\frac {1}{n} \\sum _{i = 1}^{n} (y_i - \\hat {f}(x_i))^2  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(reg)\n",
    "p <- predict(reg, RailTrail)\n",
    "error <- p - RailTrail[[\"volume\"]]\n",
    "sqrt(mean(error ^ 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Metrics)\n",
    "rmse(RailTrail$volume, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "In multiple linear regression, we have the oportunity to add new predictor variables, and it's goal is to model the effect of multiple predictors on the response (outcome) variable.\n",
    "\n",
    "\\begin{equation*}\n",
    "Y_i=\\beta_0+\\beta_1\\,X_{1}+\\beta_2\\,X_{2}+\\cdots+\\beta_p\\,X_{p}+\\epsilon_i. \\end{equation*} where $\\epsilon ~ N(0,\\sigma_\\epsilon)$\n",
    "\n",
    "\n",
    "Each new estimated coefficient ($\\hat\\beta_i$'s) is conditioned upon the other variables. Each coefficient contributes a porcentage of variation that predicts the change of y with one-unit increase in $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLR with a categorial variable \n",
    "\n",
    "If we want to understand what is the overall effect of temperature and the categorial variable of weekdays, we can create a MLR that includes both coefficients:\n",
    "\n",
    "$$\\hat{Y}=\\hat{\\beta}_0+\\hat{\\beta}_1\\,X_{1}+\\hat{\\beta}_2\\,X_{2} $$\n",
    "\n",
    "X1 is quantitative and X2 is categorical (where 1 = weekdays and 0 = weekends)\n",
    "\n",
    "Where x2 = 0 (weekdays) then we have simply:\n",
    "$$\\hat{Y}=\\hat{\\beta}_0+\\hat{\\beta}_1\\,X_{1}$$\n",
    "\n",
    "where x2 = 1\n",
    "\n",
    "$$\\hat{Y}=(\\hat{\\beta}_0+\\hat{\\beta}_2) + \\hat{\\beta}_1\\,X_{1}$$\n",
    "\n",
    "This is called a **parallel slope model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_parallel <- lm(volume ~ hightemp + weekday, data = RailTrail) \n",
    "coef(mod_parallel)\n",
    "rsquared(mod_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you interpret these coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModel(mod_parallel, system = \"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel planes: MLR with a second quantitative variable\n",
    "\n",
    "If we include in our model multiple quantitative variables, we cannot represent the model as parallel lines but as multidimentional planes which different amounts of variation being added from each predictor.\n",
    "\n",
    "following our example we can add an interesting variable that intuition predicts has a large effect on volume (Precipitation), lets create the model and calculate the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_p_planes <- lm(volume ~ hightemp + precip, data = RailTrail) \n",
    "coef(mod_p_planes)\n",
    "rsquared(mod_p_planes)\n",
    "rsquared(mod_parallel)\n",
    "rsquared(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected precipitation has a large effect on volume, we will interpret these coefficients as one increase in temperature increases volume by 6.11 riders a day (which is different that in our simple model) after controlling for the ammount of precipitation. Precipitation has a large effect, **controlling for temperature** by decreasing rail trail usage by 153 riders for each unit increase in precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions:\n",
    "\n",
    "So far we had constructed model where the predictor variables contributed to the overall response independently. We can also model the effect of the interaction of two predictors in the response variable.\n",
    "\n",
    "Our linear equation becomes:\n",
    "\n",
    "$$\\hat{Y}=\\hat{\\beta}_0+\\hat{\\beta}_1\\,X_{1}+\\hat{\\beta}_2\\,X_{2}+\\hat{\\beta}_3\\,(X_{1}*X_{2}) $$\n",
    "\n",
    "For weekends:\n",
    "$$\\hat{Y}| _{x1,x2} = 0 =\\hat{\\beta}_0+\\hat{\\beta}_1\\,X_{1}$$\n",
    "\n",
    "For weekdayes:\n",
    "\n",
    "$$\\hat{Y}| _{x1,x2} = 1 =\\hat{\\beta}_0+\\hat{\\beta}_1\\,X_{1}+\\hat{\\beta}_2*1+\\hat{\\beta}_3\\,X_{1}$$\n",
    "\n",
    "This is called an **interaction model **\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_interact <- lm(volume ~ hightemp + weekday + hightemp * weekday, data = RailTrail)\n",
    "coef(mod_interact)\n",
    "rsquared(mod_interact)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModel(mod_interact, system = \"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Interpreting Three way interactions](https://datascienceplus.com/interpreting-three-way-interactions-in-r/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the slope on weekdays is about two riders per degree higher than on weekends and holidays. This may indicate that trail users on weekends and holidays are less concerned about the temperature than on weekdays. Baumer, Benjamin S. Modern Data Science with R. Chapman & Hall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about non-linear relationships\n",
    "\n",
    "MLR works well when the variables behave in a linear manner, but in many ocassions this is not always true. There are many different variables that change their slope as they increase in value (For example, age and height). Lets model this relationship from data taken on female subjects. \n",
    "\n",
    "Lets plot the relationship between age and height using both a linear relationship or using a smoother (function LOESS). The smoother (remember kernel density plots?) allow to fit the distribution of the data much better than a linear function would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"NHANES\")\n",
    "library(NHANES) \n",
    "NHANES %>%\n",
    "sample(300) %>%\n",
    "filter(Gender == \"female\") %>% \n",
    "ggplot(aes(x = Age, y = Height)) +\n",
    "geom_point() + \n",
    "stat_smooth(method = lm, se = 0) + \n",
    "stat_smooth(method = loess, se = 0, color = \"green\") + \n",
    "xlab(\"Age (in years)\") + ylab(\"Height (in cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the fit of the smoother is much better than the fit from the linear model. However, smoothers create a buffer of possible fits that is larger than linear models. let's look at the confidence intervals of the relationship between volume and high temperature in the rail Tail dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data = RailTrail, aes(x = hightemp, y = volume)) + \n",
    "geom_point() + \n",
    "stat_smooth(method = lm) + \n",
    "stat_smooth(method = loess, color = \"green\") + \n",
    "ylab(\"Number of trail crossings\") + xlab(\"High temperature (F)\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Non-Linear Regressions and parameterization](https://datascienceplus.com/first-steps-with-non-linear-regression-in-r/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like a better fit in using the non-linear function, however, the confidence intervals, especially at the edges of the distribution is much larger (less certainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of linear models\n",
    "\n",
    "All of the coefficient estimations we have done so far relate to the estimation to the sample coefficients. We haven't said anything about the true coefficients which are unknown. \n",
    "\n",
    "We can use inference statistics in order to model each one of the distribution of the sample coefficients against the true coefficients using t-distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msummary(mod_p_planes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also assess the overall fit of our model against another sample statistic the F distribution. the F-test can assess multiple coefficients simultaneously.\n",
    "\n",
    "The hypotheses for the F-test of the overall significance are as follows:\n",
    "\n",
    "-  Null hypothesis: The fit of the intercept-only model and your model are equal.\n",
    "-  Alternative hypothesis: The fit of the intercept-only model is significantly reduced compared to your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of presenting the data is trough confidence intervals around the slope for each coefficient, Here we can say with 95% confidence that the value of the true coefficient β1 is between 4.53 and 7.69 riders per degree. That this interval does not contain zero confirms the previous hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confint(mod_p_planes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions for Regressions\n",
    "\n",
    "The inferences we made above were predicated upon our assumption that the slope follows a t-distribution. This follows from the assumption that the errors follow a normal distribution (with mean 0 and standard deviation σε, for some constant σε). Inferences from the model are only valid if the following assumptions hold:\n",
    "\n",
    "\n",
    "-  Linearity: The functional form of the relationship between the predictors and the outcome follows a linear combination of regression parameters that are correctly specified (this assumption can be verified by bivariate graphical displays).\n",
    "-  Independence: Are the errors uncorrelated? Or do they follow a pattern (perhaps over time or within clusters of subjects)?\n",
    "-  Normality of residuals: Do the residuals follow a distribution that is approximately nor- mal? This assumption can be verified using univariate displays.\n",
    "-  Equal variance of residuals: Is the variance in the residuals constant across the explana- tory variables (homoscedastic errors)? Or does the variance in the residuals depend on the value of one or more of the explanatory variables (heteroscedastic errors)? This assumption can be verified using residual diagnostics.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot(mod_p_planes, which = 1, system = \"ggplot2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a scatterplot of residuals versus fitted (predicted) values. As we observed before, the volume of crossings does not increase as much for warm temperatures as it does for more moderate ones. We may need to consider a more sophisticated model with a more complex model for temperature.\n",
    "\n",
    "It is important to careful evaluate this scatterplot of residuals vs fitted and look for non random patterns \n",
    "\n",
    "![title](residuals.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot(mod_p_planes, which = 2, system = \"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantile–quantile plot for the residuals from the regression model. The plot deviates from the straight line: This indicates that the residuals have heavier tails than a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot(mod_p_planes, which = 3, system = \"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale–location plot for the residuals from the model: The results indicate that there is evidence of heteroscedasticity (the variance of the residuals increases as a function of predicted value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing model diagnostics, it is important to identify any outliers and under- stand their role in determining the regression coefficients.\n",
    "\n",
    "-  An outlier is an observation that doesn’t seem to fit the general pattern of the data.\n",
    "-  An observation with an extreme value of the explanatory variable is a point of high leverage.\n",
    "-  A high leverage point that exerts disproportionate influence on the slope of the re- gression line is an influential point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot(mod_p_planes, which = 4, system = \"ggplot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the augment() function from the broom package to calculate the value of this statistic and identify the most extreme Cook’s distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"broom\")\n",
    "library(broom) \n",
    "augment(mod_p_planes) %>%\n",
    "filter(.cooksd > 0.4)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier corresponds to a day with nearly one and a half inches of rain (the most recorded in the dataset) and a high temperature of 84 degrees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
