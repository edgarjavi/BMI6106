{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "For the Linear Models we have evaluated so far the outcome variable has been a continous variable. In many situations we want to model a binary outcome (yes/no, disease/no_disease, etc).\n",
    "\n",
    "Let's review logistic regression by modeling the probability of developing diabetes as a function of age and BMI. \n",
    "\n",
    "First lets construct the scatterplot of diabetes status as a function of age. Then let's displays a second scatterplot of diabetes as a function of BMI (body mass index). Note that each subject can either have diabetes or not, so all of the points are displayed at zero or one on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(NHANES)\n",
    "library(ggplot2)\n",
    "library(mosaic)\n",
    "library(swirl)\n",
    "\n",
    "\n",
    "NHANES <- NHANES %>% \n",
    "mutate(has_diabetes = as.numeric(Diabetes == \"Yes\"))\n",
    "\n",
    "log_plot <- ggplot(data = NHANES, aes(x = Age, y = has_diabetes)) + \n",
    "geom_jitter(alpha = 0.1, height = 0.05) + \n",
    "geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) + \n",
    "ylab(\"Diabetes status\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_plot + xlab(\"Age (in years)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_plot + aes(x = BMI) + xlab(\"BMI (body mass index)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg <- glm(has_diabetes ~ BMI + Age, family = \"binomial\", data = NHANES) \n",
    "msummary(logreg)\n",
    "#summary(logreg)##This is the summary function from the base package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The coefficient for AGE= 0.057278 which is interpreted as the expected change in log odds for a one-unit increase in age. The odds ratio can be calculated by exponentiating this value to get 1.05895 which means we expect to see about 6% increase in the odds of being diabeitc, for a one-unit increase in age score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd = exp(0.057278)\n",
    "prob = odd/(1+odd)\n",
    "odd\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which variable is more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages <- range(~Age, data = NHANES) ##range of ages\n",
    "bmis <- range(~BMI, data = NHANES, na.rm = TRUE) ##range of BMI\n",
    "res <- 100 \n",
    "\n",
    "##Expand the range to 100 rows\n",
    "fake_grid <- expand.grid(\n",
    "    Age = seq(from = ages[1], to = ages[2], length.out = res), \n",
    "    BMI = seq(from = bmis[1], to = bmis[2], length.out = res)\n",
    ") \n",
    "\n",
    "##Get the probability of diabetis using the model logreg\n",
    "y_hats <- fake_grid %>%\n",
    "mutate(y_hat = predict(logreg, newdata = ., type = \"response\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(fake_grid)\n",
    "dim(fake_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data = NHANES, aes(x = Age, y = BMI)) + \n",
    "geom_tile(data = y_hats, aes(fill = y_hat), color = NA) + \n",
    "geom_count(aes(color = as.factor(has_diabetes)), alpha = 0.4) + \n",
    "scale_fill_gradient(low = \"white\", high = \"dodgerblue\") + \n",
    "scale_color_manual(\"Diabetes\", values = c(\"gray\", \"gold\")) + \n",
    "scale_size(range = c(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that very few young adults have diabetes, even if they have moderately high BMI scores. As we look at older subjects while holding BMI fixed, the probability of diabetes increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anova is also part of the GLM (general linear model) family. It works the same as a linear Regression but instead of one or multiple continuous predictors, In ANOVA we model the response variable (a continuous variable) as a function of one or multiple categorical variables.\n",
    "\n",
    "For example if we want to predict the duration of breastfeeding in weeks using mother's age as a predictor variable, then you would use a regression model. If you are trying to predict the duration of breastfeeding in weeks using mother's marital status (single, married, divorced, widowed), the you would use an ANOVA model. If you are trying to predict the duration of breastfeeding in weeks using prenatal smoking status (smoked during pregnancy, did not smoke during pregnancy), then you would use a two-sample t-test. If you added delivery type (vaginal/c-section) to prenatal smoking status, then the two binary predictor variables would be analyzed using an ANOVA model. [http://www.pmean.com/08/RegressionAndAnova.html](http://www.pmean.com/08/RegressionAndAnova.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANOVA models partition the variance using the sum of the squares (SS). In an ANOVA model, the total variation (total SST) is partitioned into variation between groups (between SSB) and variation within groups (within SSW). You can do the same sort of thing for a regression model, partitioning total variation into variation due to the model (model SS) and variation unexplained by the model (error SS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: μ1 = μ2 = μ3 = μ4\n",
    "H1: The means are not all equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Statistic for ANOVA\n",
    "\n",
    "We’ll then decompose the variance, as we’ve seen before in regression. The total variation measures how much the observations vary about the overall sample mean, ignoring the groups.\n",
    "\n",
    "$SST = \\sum_{i = 1}^{g} \\sum_{j = 1}^{n_i} (y_{ij} - \\bar{y})^2$\n",
    "\n",
    "$SSB = \\sum_{i = 1}^{g} \\sum_{j = 1}^{n_i} (\\bar{y}_i - \\bar{y})^2 = \\sum_{i = 1}^{g} n_i (\\bar{y}_i - \\bar{y})^2$\n",
    "\n",
    "$SSW = \\sum_{i = 1}^{g} \\sum_{j = 1}^{n_i} (y_{ij} - \\bar{y}_i)^2 = \\sum_{i = 1}^{g} (n_i - 1) s_{i}^{2}$\n",
    "\n",
    "\n",
    "$\\bar{y}_i$ is the sample mean of group  \n",
    "\n",
    "$\\bar{y}$  is the overall sample mean.\n",
    "\n",
    "$s_{i}^{2}$  is the sample variance of group  \n",
    "\n",
    "\n",
    "To develop the test statistic for ANOVA, we place this information into an ANVOA table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](ANOVA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assumptions\n",
    "\n",
    "- The observations are obtained independently and randomly from the population defined by the factor levels\n",
    "- Factor levels are normally distributed.\n",
    "- These normal populations have a common variance. (Levene’s test can be used to check this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data <- PlantGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "dplyr::sample_n(my_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(my_data$group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data$group <- ordered(my_data$group,\n",
    "                         levels = c(\"ctrl\", \"trt1\", \"trt2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "group_by(my_data, group) %>%\n",
    "  summarise(\n",
    "    count = n(),\n",
    "    mean = mean(weight, na.rm = TRUE),\n",
    "    sd = sd(weight, na.rm = TRUE)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"ggpubr\")\n",
    "# Box plots\n",
    "# ++++++++++++++++++++\n",
    "# Plot weight by group and color by group\n",
    "library(\"ggpubr\")\n",
    "ggboxplot(my_data, x = \"group\", y = \"weight\", \n",
    "          color = \"group\", palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n",
    "          order = c(\"ctrl\", \"trt1\", \"trt2\"),\n",
    "          ylab = \"Weight\", xlab = \"Treatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean plots\n",
    "# ++++++++++++++++++++\n",
    "# Plot weight by group\n",
    "# Add error bars: mean_se\n",
    "# (other values include: mean_sd, mean_ci, median_iqr, ....)\n",
    "library(\"ggpubr\")\n",
    "ggline(my_data, x = \"group\", y = \"weight\", \n",
    "       add = c(\"mean_se\", \"jitter\"), \n",
    "       order = c(\"ctrl\", \"trt1\", \"trt2\"),\n",
    "       ylab = \"Weight\", xlab = \"Treatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "boxplot(weight ~ group, data = my_data,\n",
    "        xlab = \"Treatment\", ylab = \"Weight\",\n",
    "        frame = FALSE, col = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"))\n",
    "# plotmeans\n",
    "library(\"gplots\")\n",
    "plotmeans(weight ~ group, data = my_data, frame = FALSE,\n",
    "          xlab = \"Treatment\", ylab = \"Weight\",\n",
    "          main=\"Mean Plot with 95% CI\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the analysis of variance\n",
    "res.aov <- aov(weight ~ group, data = my_data)\n",
    "# Summary of the analysis\n",
    "summary(res.aov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple pairwise-comparison between the means of groups\n",
    "\n",
    "The ANOVA summary result shows a significant different in the model, however, we don't know which groups contribute more to this difference. It’s possible to perform multiple pairwise-comparison, to determine if the mean difference between specific pairs of group are statistically significant.\n",
    "\n",
    "## Tukey multiple pairwise-comparisons\n",
    "As the ANOVA test is significant, we can compute Tukey HSD (Tukey Honest Significant Differences, R function: TukeyHSD()) for performing multiple pairwise-comparison between the means of groups.\n",
    "\n",
    "The function TukeyHD() takes the fitted ANOVA as an argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TukeyHSD(res.aov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on multiple pairwise comparisons\n",
    "\n",
    "In many ocasions we want to test multiple independent or even dependent statistical tests on the same data set (comparing multiple variables, multiple genes, different categorical levels, etc). Why is this a problem?\n",
    "\n",
    "Let's say you are testing 20 different hypothesis on the same data set, and a significance level of 0.05. What’s the probability of observing at least one significant result just due to chance?\n",
    "\n",
    "            P (at least one significan result)  = 1 − P(no significant results)\n",
    "\n",
    "$$ = 1 (1-0.05)^{20} $$\n",
    "$$ = 0.64 $$\n",
    "\n",
    "So, with 20 tests being considered, we have a 64% chance of observing at least one significant result, even if all of the tests are actually not significant. In genomics and other biology-related fields, it’s not unusual for the number of simultaneous tests to be quite a bit larger than 20... and the probability of getting a significant result simply due to chance keeps going up.\n",
    "\n",
    "The Bonferroni correction is an adjustment made to P values when several dependent or independent statistical tests are being performed simultaneously on a single data set. To perform a Bonferroni correction, divide the critical P value (α) by the number of comparisons being made. For example, if 10 hypotheses are being tested, the new critical P value would be α/10. The statistical power of the study is then calculated based on this modified P value.\n",
    "\n",
    "The Bonferroni correction is used to reduce the chances of obtaining false-positive results (type I errors) when multiple pair wise tests are performed on a single set of data. Put simply, the probability of identifying at least one significant result due to chance increases as more hypotheses are tested.\n",
    "\n",
    "For more info see [https://www.stat.berkeley.edu/~mgoldman/Section0402.pdf](https://www.stat.berkeley.edu/~mgoldman/Section0402.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions:\n",
    "\n",
    "1. Homogeneity of variances: Similar to linear models we can check the residuals vs fitted values plot and look for departures from homogeneity. It’s also possible to use Bartlett’s test or Levene’s test to check the homogeneity of variances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Homogeneity of variances\n",
    "plot(res.aov, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(car)\n",
    "leveneTest(weight ~ group, data = my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2.$ Normality: We can use a qqplot to evaluate departures from linearity. We can use the Shapiro-Wilk test on the ANOVA residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Normality\n",
    "plot(res.aov, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the residuals\n",
    "aov_residuals <- residuals(object = res.aov )\n",
    "# Run Shapiro-Wilk test\n",
    "shapiro.test(x = aov_residuals )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "There are many more models in the family of General Linear Models, and model selection depends on the type of data you are dealing with.\n",
    "\n",
    "![title](GLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to not that in many cases we don't which variables need to be added to the model (remember the more variables the larger the samples sizes need to be). There are multiple techniques for variables selection, which we will not cover in this course but that are important to study and apply.\n",
    "\n",
    "Many of these techniques rely on a systematic inclusion and exclusion of variables to the model and evaluation using the AIC **Akaike information criterion** [https://en.wikipedia.org/wiki/Akaike_information_criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) or the BIC **Bayesian information criterion**.\n",
    "\n",
    "Recomended book on GLM:\n",
    "\n",
    "Beginner's Guide to GLM and GLMM with R (Zuur et al, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
