{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Non-parametric statistics (AKA Distribution Free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We evaluated some non-parametric estimations in previous sections while looking at descriptive statistics (Histograms, Kernel density plots). The advantage of non-paramatric statistics is that they are distribution free or at least the distribution parameters are unspecified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What to do when your data has outliers or does not fit the assumptions of any test statistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1.  Analyze data with and then without the outlier values and see how outliers affect the results.  These analyses should be discussed in the result and discussion sections.  You cannot ignore outliers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2.  Categorize data and re-analyze.  For example, a measurement of physical activity is the MET – metabolic equivalent of task.  Many people (in one study 53%) score a 0.  Clearly this distribution of the variable will not be close to normal so parametric statistics will not be applicable.  We can categorize MET into levels (none, medium and high activity) and use tests that can be used with categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3.  Use non-parametric methods, a few of which are discussed in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WHAT ARE NON-PARAMETRIC METHODS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus far, we have assumed that X has a known distribution with hypothesized population parameters. \n",
    "i.e.  If a quantitative, continuous variable, then our assumptions have been that X~N(μ, σ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "in parametric analysis the data is fit to a normal (gaussian) model, nonparametric methods are distribution free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nonparametric methods don't relay on parameter comparisons such as mean, variance etc to compare groups or distributions. \n",
    "\n",
    "However,The cost of fewer assumptions is that non-parametric tests are generally less powerful.  That means that you are more likely to be able to detect a significant effect when one truly exists.\n",
    "\n",
    "The advantage of non-parametric tests is that it may be the only way to analyze some types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WHEN TO USE NON-PARAMETRIC METHODS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### •\tIf the distribution of X is not known (especially if the sample size is small) or the distribution does not 'fit' any of the 'known' distributions such as the distributions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](Dsitributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another way to evaluate if the data follows a normal distribution is checking the kurtosis and skewness.\n",
    "\n",
    "Skewness is usually described as how symetrical is a distribution.\n",
    "\n",
    "Kurtosis evaluates the tails of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](kurtosis_skeweness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### •\tWhen the outcome is an ordinal variable or a rank variable.  \n",
    "For example, if you are interested in the ability of people to perform independently in six basic activities of daily living with total responses of 0 - 6, where 0 means they are able to perform all six activities to 6 meaning they can perform none of the activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### •\tWhen there are definite outliers.  \n",
    "For example, days spent in a hospital after a surgical procedure.  Most patients fall within a short, specified time, but there will always be some patients who require extra days, and rarely, some may require a very long stay.  These outliers contain important information - they cannot be ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### •\tWhen the outcome has clear limits of detection. \n",
    "For example, levels of 1,2,4-trichlorobenzene (1,2,4-TCB) in blood cannot be detected by todays techniques below 0.21 ug/L (https://pubchem.ncbi.nlm.nih.gov/compound/1_2_4-trichlorobenzene).  That does not mean serum levels < 0.21 ug/L, only that it cannot be detected.  Therefore, any analysis of serum levels of 1,2,4-TCB will have an artificial left limit of 0.21 ug/L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### •\tWhen the sample size is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In parametric methods, the measure of central location used was the mean.  \n",
    "\n",
    "Nonparametric statistics are normally based on Ranks of the data sets, where in most non-parametric, or distribution free tests, the measure of central location used is the median.\n",
    "\n",
    "Recall, by definition:\n",
    "\n",
    "\tP(X < M) = .5 and P(X > M) = .5\n",
    "\n",
    "That is, for a quantitative, continuous random variable, the median is defined as the point M such that 50% of the time x lies below M and 50% of the time above M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Parametric Tests\n",
    "Many parametric tests have a non-parametric counterpart.  A few non-parametric tests related to what you have learned thus far are:\n",
    "\n",
    "| **Parametric tests (means)** | **Nonparametric tests (medians)** |\n",
    "| --- | --- |\n",
    "| 1-sample t test | 1-sample Sign test |\n",
    "| 2-sample t test | Mann-Whitney test |\n",
    "| 2-sample paired t test | Wilcoxon signed-rank test |\n",
    "| ANOVA | Kruskal Wallis test |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sign Test\n",
    "\n",
    "The idea of this test is simple, the null hypothesis states that the median of the dataset is the same as an hypothesized median (you can set it up based on data or on prediction). The alternative hypothesis is that the dataset does not conform(equal) to a hypothesized value.\n",
    "\n",
    "The way to use the test is very simple.\n",
    "\n",
    "1. order the data numerically\n",
    "2. count how many data points are above and below the hypothesized value\n",
    "3. Compute the p-value using the binomial distribution (you can do one tail or two tails)\n",
    "4. Don't forget to remove first the ties with the hypothesized median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at an example, \n",
    "we have collected annual incomes to ramdon people and we want to know if this population has an income equal to $30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "incomes <- c(8478, 21564, 36562, 176602, 9395, 18320,30000, 50000, 2, 40298, 39, 10780, 2268583, 3404930)\n",
    "par(mfrow=c(1,2))\n",
    "hist(incomes)##data skwewed\n",
    "qqnorm(incomes)\n",
    "qqline(incomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sort(incomes)\n",
    "##how many incomes are larger and smaller than 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b <- sum(incomes > 30000)\n",
    "b\n",
    "\n",
    "n <- sum(incomes != 30000)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "##N = 13 and \n",
    "binom.test(3, n = 12, p = 0.5)##pvalue and CI for two sided.\n",
    "pbinom(3,13,0.5)##This will get you the pvalues a one side with the alternative greater\n",
    "sum(dbinom(3:13,12,0.5))##This will get you the pvalues a one side with the alternative smaller\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mann-Whitney U test\n",
    "\n",
    "This thest is the equivalent to the 2 independent sample t-test. The null hypothesis is that the median of the two samples is the same.\n",
    "\n",
    "To perform this test you:\n",
    "\n",
    "1. Sort each samples\n",
    "2. assign ranks starting from the smaller value from both samples\n",
    "3. if both samples have the same value the rank assigned will be its location plus 0.5\n",
    "4. sum up the ranks for each one the two samples\n",
    "5. Calculate the U statistics for each sample with the following equation. The U value is the smaller of the two numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](U_statistic.png)\n",
    "\n",
    "Where R is the rank sum for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "6. Find the critical value and compare it to the U statistics using the U table (the Mann-Whitney-Table-CriticalValues.pdf in this folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#example taken from http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_nonparametric/BS704_Nonparametric4.html\n",
    "\n",
    "A new approach to prenatal care is proposed for pregnant women living in a rural community. The new program involves in-home visits during the course of pregnancy in addition to the usual or regularly scheduled visits. A pilot randomized trial with 15 pregnant women is designed to evaluate whether women who participate in the program deliver healthier babies than women receiving usual care. The outcome is the APGAR scoretext annotation indicator measured 5 minutes after birth. Recall that APGAR scores range from 0 to 10 with scores of 7 or higher considered normal (healthy), 4-6 low and 0-3 critically low. The data are shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Usual_Care = c(8,7,6,2,5,8,7,3)\n",
    "New_Program = c(9,9,7,8,10,9,6)\n",
    "\n",
    "n1 = 8\n",
    "n2 = 7\n",
    "\n",
    "R1 = 45.5\n",
    "R2 = 74.5\n",
    "\n",
    "#Looking at the U table the critical value for n1 = 7 and n2 = at alpha 0.05 is 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](Utest_Example_Ranks.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "U1 = (n1*n2)+(n1*(n1+1)/2)-R1\n",
    "U2 = (n1*n2)+(n2*(n2+1)/2)-R2\n",
    "U1\n",
    "U2 \n",
    "\n",
    "#The test statistic is U=9.5\n",
    "\n",
    "#As 9.5 < than 10 then we reject the null hypothesis at 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "##With R you can get a p-value - W is a little bit different as this function evaluates ties a little different\n",
    "\n",
    "W = data.frame(Data= Usual_Care)\n",
    "W$Treat = \"Usual\"\n",
    "\n",
    "W2 = data.frame(Data = New_Program)\n",
    "W2$Treat = \"New\"\n",
    "\n",
    "W_F = rbind(W,W2)\n",
    "\n",
    "wilcox.test(Data ~ Treat, data=W_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wilcoxon signed-rank test\n",
    "\n",
    "This thest is the equivalent to the 2 dependent sample t-test. The null hypothesis is that the median of the two samples is the same.\n",
    "\n",
    "To perform this test you:\n",
    "\n",
    "1. Sort each samples\n",
    "2. assign ranks starting from the smaller value from both samples\n",
    "3. if both samples have the same value the rank assigned will be its location plus 0.5\n",
    "4. sum up the ranks for each one the two samples\n",
    "5. Calculate W+ and W- with wi = ni(ni+1)/2 (n is each one of the sample sizes) - and choose the smallest W\n",
    "6. Get the W critical value from the W table (wilcox_signrank_table.pdf) and compare it with the W statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example (From Handbook of Biological Statistics - John H. McDonald):\n",
    "\n",
    "Laureysens et al. (2004) measured metal content in the wood of 13 poplar clones growing in a polluted area, once in August and once in November. Concentrations of aluminum (in micrograms of Al per gram of wood) are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Input = (\"\n",
    " Clone          August  November\n",
    " Balsam_Spire    8.1    11.2\n",
    " Beaupre        10.0    16.3\n",
    " Hazendans      16.5    15.3\n",
    " Hoogvorst      13.6    15.6\n",
    " Raspalje        9.5    10.5\n",
    " Unal            8.3    15.5\n",
    " Columbia_River  18.3   12.7\n",
    " Fritzi_Pauley   13.3   11.1\n",
    " Trichobel        7.9   19.9\n",
    " Gaver            8.1   20.4\n",
    " Gibecq           8.9   14.2\n",
    " Primo           12.6   12.7\n",
    " Wolterson       13.4   36.8\n",
    "\")\n",
    "\n",
    "Data = read.table(textConnection(Input),header=TRUE)\n",
    "Data\n",
    "wilcox.test(Data$August,\n",
    "            Data$November,\n",
    "            paired=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot(Data$August, Data$November,\n",
    "     pch = 16,\n",
    "     xlab=\"August\",\n",
    "     ylab=\"November\")\n",
    "\n",
    "abline(0,1, col=\"blue\", lwd=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kruskal Wallis test\n",
    "\n",
    "This thest is the equivalent to an ANOVA test. where you have a dependent continuous variable and a independent ordinal or discrete variable\n",
    "\n",
    "To perform this test you:\n",
    "\n",
    "1. Sort each samples\n",
    "2. assign ranks starting from the smaller value from both samples\n",
    "3. if samples have the same value the rank assigned will be its location plus 0.5\n",
    "4. sum up the ranks for each one the samples\n",
    "5. Calculate the H statistics as below, where g is each one of the groups from the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](H_Statistics.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example from: http://courses.atlas.illinois.edu/spring2016/STAT/STAT200/RProgramming/NonParametricStats.html\n",
    "\n",
    "16 students were randomly assigned 3 different Final exams, were all 3 versions of the exam equally difficult?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "final <- c(10,60,70,80,100, 50,70,81,85,95, 20,75,86,90,98,99)\n",
    "group <- factor(c(rep(\"A\",5), rep(\"B\",5), rep(\"C\",6)), levels=c(\"A\",\"B\",\"C\"))\n",
    "# Check...\n",
    "# Group A's scores\n",
    "final[group==\"A\"]\n",
    "\n",
    "##R is the sum of the ranks for each group\n",
    "R <- tapply(rank(final), group, sum)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "boxplot(final~group,main=\"3 Different Test Scores\",\n",
    "   xlab=\"Groups\", ylab=\"Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N <- length(final) # total number of scores\n",
    "n <- table(group) # number of observations in each group\n",
    "ExpR <- n*(N+1)/2 # expected rank sum for each group\n",
    "H <- 12/(N*(N+1))*sum( (R-ExpR)^2/n )\n",
    "H ##H statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate the p-value using the chi2 statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "g = length(levels(group))\n",
    "\n",
    "pchisq(H, g-1, lower.tail=FALSE) #Conclusion: The p-value is greater than 5%, so we do not reject the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kruskal.test(final ~ group)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
